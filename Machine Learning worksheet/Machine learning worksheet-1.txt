                          
                   MACHINE LEARNING     WORKSHEET – 1

In Q1 to Q7, only one option is correct, Choose the correct option:

1. The value of correlation coefficient will always be:
A) between 0 and 1 B) greater than -1
C) between -1 and 1 D) between 0 and -1

Ans:- C) between -1 and 1

2. Which of the following cannot be used for dimensionality reduction?
A) Lasso Regularisation B) PCA
C) Recursive feature elimination D) Ridge Regularisation

Ans:- Ridge Regualarisation

3. Which of the following is not a kernel in Support Vector Machines?
A) linear B) Radial Basis Function
C) hyperplane D) polynomial

Ans:- C) hyperplane

4. Amongst the following, which one is least suitable for a dataset having non-linear decision boundaries?
A) Logistic Regression B) Naïve Bayes Classifier
C) Decision Tree Classifier D) Support Vector Classifier

Ans:- A) Logistic Regression

5. In a Linear Regression problem, ‘X’ is independent variable and ‘Y’ is dependent variable, where ‘X’ represents
weight in pounds. If you convert the unit of ‘X’ to kilograms, then new coefficient of ‘X’ will be?
(1 kilogram = 2.205 pounds)
A) 2.205 × old coefficient of ‘X’ B) same as old coefficient of ‘X’
C) old coefficient of ‘X’ ÷ 2.205 D) Cannot be determined

Ans:- A) 2.2.05  x old coefficient of "X"

6. As we increase the number of estimators in ADABOOST Classifier, what happens to the accuracy of the model?
A) remains same B) increases
C) decreases D) none of the above

Ans:- B) increase

7. Which of the following is not an advantage of using random forest instead of decision trees?
A) Random Forests reduce overfitting
B) Random Forests explains more variance in data then decision trees
C) Random Forests are easy to interpret
D) Random Forests provide a reliable feature importance estimate

Ans:- C) Random Forests are easy to interpret

In Q8 to Q10, more than one options are correct, Choose all the correct options:

8. Which of the following are correct about Principal Components?
A) Principal Components are calculated using supervised learning techniques
B) Principal Components are calculated using unsupervised learning techniques
C) Principal Components are linear combinations of Linear Variables.
D) All of the above

Ans:- A) Principal Components are calculated using supervised learning techniques 
      C) Principal Components are linear combinations of linear variables  

9. Which of the following are applications of clustering?
A) Identifying developed, developing and under-developed countries on the basis of factors like GDP, poverty
index, employment rate, population and living index
B) Identifying loan defaulters in a bank on the basis of previous years’ data of loan accounts.
C) Identifying spam or ham emails
D) Identifying different segments of disease based on BMI, blood pressure, cholesterol, blood sugar levels.

Ans:- C) Identifying spam or ham emails
          D) Identifying different segments of disease based on RBI, blood pressure, cholestrol, blood sugar levels. 

10. Which of the following is(are) hyper parameters of a decision tree?
A) max_depth B) max_features
C) n_estimators D) min_samples_leaf

Ans:- A) max_depth
      B) max_features 
      D) mis_samples_leaf


Q10 to Q15 are subjective answer type questions, Answer them briefly.


11. What are outliers? Explain the Inter Quartile Range(IQR) method for outlier detection.

Ans:-  Outliers :- An outlier is an object that deviates significantly from the rest of the objects. They can be caused by measurement or 
                           execution error. The analysis of outlier data is referred to as outlier analysis, it is help ful to have a rule to apply when determining
                           whether a data point is truly sn outlier, this is where the interquartile range rule is comes in.
 
Interquatile range :- Any set of data can be 5 number summary,which can give you information you need to find patterns and outliers.
                             1) The minimum or lowest value of the dataset
                             2) The first quartile Q1, which represents a quarter of the way through the list of all data.
                             3) The median of the dataset, which represents the midpoint of the whole list of data.
                             4) The third quartile Q3, which represents 3 quarters of the way through the list of all data
                             5) The maximum or highest value of the dataset.
These 5 numbers tell a person more about their data than looking at the numbers all at once could , or at least make this much easier.
All you do to find it is subtract the first quartile from the third quartile
IQR = Q3 - Q1

Using  IQR to find Outliers:-
1) calculaate the interquartile range for data
2) Multiply the interquartile range by 1.5 
3) Add 1.5 x (IQR) to the third quartile. Any number greater than this is a suspected outlier
4) Subtract 1.5 x(IQR) from the first quartile. Any number less than this is a suspected outlier.

Any potential outlier obtained by the interquartiler method should be examined in the context of the entire set of data.

12. What is the primary difference between bagging and boosting algorithms?

Ans:- Bagging and Boosting are similar in that they are both ensemble techniques, where a set of weak learners are combined to create a strong leraner that
obtains better performance that a single one. 

▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
▐                       Bagging                                            ▐                                 Boosting                                   ▐
▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
▐   Simplest way of combining predictions that belogns to the same type.   ▐     A way of combining predictions that belong to the different types      ▐
▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
▐   Aim to decrease variance, not bias.                                    ▐     Aim to decrease bias not variance.                                     ▐
▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
▐   Each model is built independently.                                     ▐     New models are influenced by performance of previously built models    ▐
▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
▐   Different training data suibsets are randomly drwan with replacement   ▐     Every new subsets contains the elements that were misclassified by     ▐
▐   from the entire training dataset                                       ▐     previous model                                                         ▐
▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
▐   Bagging tries to solve over-fitting probelm                            ▐     Boosting tries to reduce bias.                                         ▐
▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
▐   If the classifier is unstable, then apply bagging.                     ▐     If the classifier is stable and simple, then apply boosting            ▐
▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬

13. What is adjusted R2 in logistic regression. How is it calculated?

Ans:- The adjusted R squared compares the explanatory power of regression models that contains different numbers of predictors.
Suppose we compare  a five-predictor model with higher R-squared to a one-predictor model. Does the five predictor model 
have ahigher R squared because it's better.
The adjusted R squared is a modified version of R squared that has been adjusted for the number of predictors in the model. The adjusted
R squared increasesonly if the new term improves the model more than would be expected by chance. it decreases when a predictor improves the model by less than 
expected by chance. the adjusted R squared  can be negative . but its usually not. It is always better than the R squared.

Adjusted R squared tries to measure the proportion of the variation which is explained by those independent variables that helps in explaining the dependents variables. 
It discipline you for adding independent variables that do not actually help in predicting the dependent variables.

Adjusted R squared could be calculated based on the value of R squared, number of independent variable which are predictors and total sample size:

Adjusted R-squared = 1-(1-R2)(N-1) / N-p-1

where R2 = sample R squared
           N = Total sample size
           p = number of predictors


14. What is the difference between standardisation and normalisation?

Ans:-   Feature scaling:- One of the most important data preprocessing step in machine learning. Algorithms that compute the distance between the featuress are biased 
towards numerically larger values if the data is not scaled.
There are somt feature scaling techniques such as Normalisation & Standardisation that are the most popular and at the same time, the most confusing ones.

Noramlization or Min-Max Scaling :-  Used to transform features to be on a similar scale. The new points is calulated as  X_new = (X - X_min) / (X_max - X_min)

Standardization or Z- score Normalization :- Used to transform features by subtracting from mean and dividing by standard deviation, and it is calculated as 
                                                                        X_new = (X - mean) / std

             
Difference between:- 
       ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
       ▐                   Noramlization                                  ▐                       Standarsization                                              ▐
       ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
       ▐   Minimum & maximum value of features are used for scaling       ▐           Mean & standard deviation is used for scaling                            ▐
       ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
       ▐   It is used when features are of different scales               ▐           It is used when we want to ensure zero mean and unit standard deviation  ▐ 
       ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
       ▐   Scales values between [0,1] or [-1,1]                          ▐           It is not bounded to a certain range.                                    ▐ 
       ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
       ▐   It is really affected by outliers                              ▐           It is much less affected by outliers.                                    ▐
       ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬  
       ▐   Scikit learn provides a transformer called MinMaxScaler        ▐           Scikit learn provides a transformer called StandardScaler for            ▐
       ▐   for Normalization                                              ▐           Standardization.                                                         ▐  
       ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬
       ▐   It is useful when we don't know about the distribution         ▐           It is useful when the feature distribution is normal or gaussian.        ▐
       ▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬▬


15. What is cross-validation? Describe one advantage and one disadvantage of using cross-validation.

Ans:- In machine learning, we couldn't fit model on the training data and can't say that the model will work accurately for the real data. For this , we must assure that our model 
got the correct patterns from the data, and it is not getting up too much noises. For this purpose , we use the Cross Validation technique.

Cross Validation :-  It is technique in which we train our model using the subset od the datset and then evaluate using the complementaryt subset of the dataset. usually 3 steps involved in cross 
validation are as follows:- 1) Reserve some portion of sample dataset.
                            2) Using the rest dataset train the model.
                            3) Test the model using the reserve portion of the dataset.
Methods of Cross validation :- 1) Validation 
                               2) LOOCV(Leave one out cross ccalidation)
                               3) K-fold cross validation

Advantages:-  1) Reduces Overfitting :-  In this we split dataset into multiple folds and train the algorithm on diffrent folds. this prevents our model from overfitting the training dataset. 
              2) Hperparameter tuning :- CV helps in finding the optimal value of hyperparameters to increase the efficiency of the algorithm.

Disadvantage :- 1) Increase Training Time :- CV drastically increase the training time. Earlier you had to train  your model only on one training set, but with CV you have to train your model on multiple training sets.
                2) Needs Expensive Computation:- CV is computational very expensive in terms of processing power required.

